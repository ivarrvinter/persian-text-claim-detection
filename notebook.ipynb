{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CrcrtsBtbRx"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "hkLaIdpVmguC",
        "outputId": "b7052858-9556-42ce-f52c-5aa757361443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "  Obtaining dependency information for hazm from https://files.pythonhosted.org/packages/e9/16/66fa99ead559a36d56d3a8a9f8b77f1fb03cf6d91d338559325cfe67860d/hazm-0.9.3-py3-none-any.whl.metadata\n",
            "  Using cached hazm-0.9.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
            "  Using cached fasttext_wheel-0.9.2-cp311-cp311-win_amd64.whl (232 kB)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from hazm) (4.3.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from hazm) (1.24.3)\n",
            "Collecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Using cached python-crfsuite-0.9.9.tar.gz (440 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from hazm) (1.3.0)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm)\n",
            "  Obtaining dependency information for pybind11>=2.2 from https://files.pythonhosted.org/packages/06/55/9f73c32dda93fa4f539fafa268f9504e83c489f460c380371d94296126cd/pybind11-2.11.1-py3-none-any.whl.metadata\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (65.5.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.11.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
            "Requirement already satisfied: click in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.6)\n",
            "Requirement already satisfied: joblib in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.65.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ivarr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk<4.0.0,>=3.8.1->hazm) (0.4.6)\n",
            "Using cached hazm-0.9.3-py3-none-any.whl (367 kB)\n",
            "Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Building wheels for collected packages: python-crfsuite\n",
            "  Building wheel for python-crfsuite (setup.py): started\n",
            "  Building wheel for python-crfsuite (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for python-crfsuite\n",
            "Failed to build python-crfsuite\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  √ó python setup.py bdist_wheel did not run successfully.\n",
            "  ‚îÇ exit code: 1\n",
            "  ‚ï∞‚îÄ> [12 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-311\n",
            "      creating build\\lib.win-amd64-cpython-311\\pycrfsuite\n",
            "      copying pycrfsuite\\_dumpparser.py -> build\\lib.win-amd64-cpython-311\\pycrfsuite\n",
            "      copying pycrfsuite\\_logparser.py -> build\\lib.win-amd64-cpython-311\\pycrfsuite\n",
            "      copying pycrfsuite\\__init__.py -> build\\lib.win-amd64-cpython-311\\pycrfsuite\n",
            "      running build_ext\n",
            "      building 'pycrfsuite._pycrfsuite' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for python-crfsuite\n",
            "ERROR: Could not build wheels for python-crfsuite, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug6TbjjCodtT",
        "outputId": "fbf733ad-6dc9-4570-90f5-d295682886ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDvPmUCM6wVG",
        "outputId": "17b93e21-8306-4609-d6d7-623f34a812c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_Vqr7mP4ohHA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ivarr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# from hazm import Normalizer, word_tokenize\n",
        "import emoji\n",
        "from transformers import AutoTokenizer, TFBertModel\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, LayerNormalization\n",
        "# from keras import activations\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import class_weight\n",
        "from keras import regularizers\n",
        "from keras.losses import CategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkyxv-9K4dLq"
      },
      "source": [
        "## Reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0UAa73jo4Y5",
        "outputId": "a8e66cdf-eb44-4ecb-f7ce-11ad51f0440d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DyqMkQ-prgAV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('dataset/2.train_data_text.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0fo5jSeteew"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10eS_lR64nyE"
      },
      "source": [
        "### Demojizing emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v5SG7SUTK2aU"
      },
      "outputs": [],
      "source": [
        "df['data'] = df['data'].replace(r'üí®', r' ', regex=True)\n",
        "df['data'] = df['data'].replace(r'üòå', r' ', regex=True)\n",
        "df['data'] = df['data'].replace(r'‚ñ™Ô∏è', r' ', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jcJRIfOEulpM"
      },
      "outputs": [],
      "source": [
        "df['data_demojized'] = df['data'].apply(lambda x: emoji.demojize(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmBHBpQr4vnr"
      },
      "source": [
        "### Cleaning texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FFpHa4BcspbT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "MAKE SURE TO RETRAIN THE MODEL AGAIN\n",
        "WITHOUT DELETING HASHTAGS' CONTENT\n",
        "SOME OF THEM ARE MEANINGFUL\n",
        "SPECIALLY THOSE IN THE TEXT\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rules = [\n",
        "    r':\\s+:',\n",
        "    r':#', # removing certain # from sentence carring zero-width space\n",
        "    r'#', # removing hashtag for texts like #ŸÖÿ¥ÿßŸá€åÿ±_ÿß€åÿ±ÿßŸÜ and #ŸÖÿ¥ÿßŸá€åÿ±_ÿß€åÿ±ÿßŸÜ_Ÿà_ÿ¨ŸáÿßŸÜ changed #\\w+ to #\n",
        "    r'\\(\\w{1}\\)', # removing (ÿπ) and (ÿµ)\n",
        "    r'\\.$', # removing period at the end of the line\n",
        "    r'ÿ±Ÿá', # removing ÿ±Ÿá from ÿßŸÖÿßŸÖ ÿ±Ÿá\n",
        "    r'\\[...\\]', # removing three [periods] like [...] ŸÖÿ´ŸÑ ÿ¢ŸÇÿß€å\n",
        "    # r'#\\w+\\u200c\\w+', # removing hashtag coming with zero width non-joiner character like #ÿ™ŸÑ‚Äåÿ¢Ÿà€åŸà\n",
        "    r'\\.{2,}', # removing extra periods like .. and ...\n",
        "    r'[\\ÿå\\ÿõ\\,\\!\\¬´\\¬ª\\(\\)\\'\\\"\\‚Äú\\‚Äù\\_]', # removing Persian punctuations\n",
        "    r'\\:\\w+(_\\w+)*\\:', # removing demojized emojis in formats like :red_circle\n",
        "    r'\\\\U[0-9a-zA-Z]{4,8}', # removing demojized emojis in unicode \\Uxxxxxxxx formats NOT FOUND\n",
        "    r'\\w\\:', # removing colons\n",
        "    r'[a-zA-z]{3,}', # removing emoji-remained characters\n",
        "    r'(?<=ÿ™ÿ£€å€åÿØ|ÿ™ÿß€å€åÿØ)',\n",
        "    r'(?=ÿØÿ±)',\n",
        "    r'(?<=\\w)\\.(?=\\s+)', # removing period from the middle of the line at the end-of-sentence\n",
        "    r'ÿü',\n",
        "    r'(?<=%)'\n",
        "    ]\n",
        "df['data_clean'] = df['data_demojized'].replace(rules, r' ', regex=True)\n",
        "\n",
        "extra_rules = [\n",
        "    r'(?=Ÿáÿß|Ÿáÿß€å)'\n",
        "    ]\n",
        "df['data_clean'] = df['data_clean'].replace(extra_rules, '\\u200c', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zs-Ifw04j2k"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDEE98rstxVs"
      },
      "outputs": [],
      "source": [
        "normalizer = Normalizer()\n",
        "df['data_normalized'] = df['data_clean'].apply(lambda x: normalizer.normalize(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVuQSjzp9DMz"
      },
      "source": [
        "##### Testing text cleaning and normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kVyb__9z4Pe"
      },
      "outputs": [],
      "source": [
        "no = 2836\n",
        "print(df['data'][no])\n",
        "print(df['data_clean'][no])\n",
        "print(df['data_normalized'][no])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1vyShMLFXab"
      },
      "outputs": [],
      "source": [
        "text = 'ŸÑÿßÿ±€åÿ¨ÿßŸÜ€å ÿ±ÿ¶€åÿ≥ ŸÖÿ¨ŸÖÿπ ÿ™ÿ¥ÿÆ€å ÿßÿ≤ ÿ≠ÿ∂Ÿàÿ± ÿß€åŸÜ‚Äåÿ¨ÿßŸÜÿ® ÿØÿ± ÿ¥Ÿàÿ±ÿß€å ŸÜ⁄ØŸáÿ®ÿßŸÜ ÿØÿ± ÿ≥ÿßŸÑ €±€≥€∏€∞ Ÿá€å⁄Ü‚Äå⁄ØÿßŸá ÿ™ÿµŸÖ€åŸÖÿßÿ™ ÿ¥Ÿàÿ±ÿß ÿ±ÿß ÿ™ÿß ÿß€åŸÜ ÿ≠ÿØ ÿ∫€åÿ±ŸÇÿßÿ®ŸÑ ÿØŸÅÿßÿπ ŸÜ€åÿßŸÅÿ™Ÿá‚ÄåÿßŸÖ ⁄ÜŸá ÿØÿ± ÿ™ÿ£€å€åÿØ ÿµŸÑÿßÿ≠€åÿ™‚ÄåŸáÿß Ÿà ⁄ÜŸá ÿØÿ± ÿπÿØŸÖ‚Äåÿßÿ≠ÿ±ÿßÿ≤ ÿµŸÑÿßÿ≠€åÿ™‚ÄåŸáÿß. ÿ®ÿß ŸÜÿ∏ÿßÿ±ÿ™ ÿßÿ≥ÿ™ÿµŸàÿßÿ®€å Ÿà ÿ¥Ÿàÿ±ÿß€å ŸÜ⁄ØŸáÿ®ÿßŸÜ €å⁄©‚ÄåÿØÿ≥ÿ™ ÿßÿµŸàŸÑ⁄Øÿ±ÿß ÿ¢ÿ¥ ŸáŸÖÿßŸÜ ÿ¢ÿ¥ Ÿà ⁄©ÿßÿ≥Ÿá ŸáŸÖÿßŸÜ ⁄©ÿßÿ≥Ÿá ÿßÿ≥ÿ™'\n",
        "for rule in rules:\n",
        "    print(re.sub(rule, r' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZVIEQ83NjH0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        " Testing data\n",
        "\n",
        "'''\n",
        "print(df[df['data_normalized'].str.contains(r'\\.(?=\\s\\w)', regex=True)][['data_normalized']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq_rHBlh9NjB",
        "outputId": "ad49d127-7324-4ae2-c0de-360f08f3622e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ÿ≥ÿ±ŸÖÿß€åŸá‚ÄåŸáÿß€å Ÿàÿ∑ŸÜ€å\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"ÿ≥ÿ±ŸÖÿß€åŸáŸáÿß€å Ÿàÿ∑ŸÜ€å\"\n",
        "\n",
        "# Add a space before \"Ÿáÿß\" or \"Ÿáÿß€å\"\n",
        "modified_text = re.sub(r'(?=Ÿáÿß|Ÿáÿß€å)', '\\u200c', text)\n",
        "\n",
        "print(modified_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp_907er47lw"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyMtXmTCYtXo"
      },
      "outputs": [],
      "source": [
        "df['data_tokenized'] = df['data_normalized'].apply(lambda x: word_tokenize(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWQobiKu4-kG"
      },
      "source": [
        "### Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rL84RSBUXKQ"
      },
      "outputs": [],
      "source": [
        "stopwords = [\n",
        "    \"ÿßÿ≤\", \"ÿ®Ÿá\", \"⁄©Ÿá\", \"ÿØÿ±\", \"ÿ®ÿß\", \"ÿß€åŸÜ\", \"ÿ®ÿ±ÿß€å\", \"ÿ±ÿß\", \"ŸáŸÖ\", \"ÿ™ÿß\", \"ÿ¢ŸÜ\", \"€åÿß\", \"ÿßÿ≥ÿ™\", \"€å⁄©\", \"ŸÜ€åÿ≤\", \"ÿßŸÖÿß\",\n",
        "    \"Ÿáÿ±\", \"ÿ®ŸàÿØ\", \"ÿ®ÿßÿ¥ÿØ\", \"ŸÖÿß\", \"ÿ¥ŸÖÿß\", \"ÿßŸà\", \"ÿ¢ŸÜŸáÿß\", \"ÿ™Ÿà\", \"ŸÖŸÜ\", \"ÿß⁄Øÿ±\", \"ÿ®€å\", \"ÿßŸàŸÜ\", \"ÿ±Ÿà\", \"ÿ®ŸàÿØŸÖ\", \"ŸÜŸá\", \"ÿ®ÿßÿ¥Ÿá\",\n",
        "    \"ÿ¥ÿØŸá\", \"ÿßŸàŸÜÿß\", \"Ÿáÿ≥ÿ™\", \"Ÿáÿ≥ÿ™ŸÖ\", \"ÿÆŸàÿØ\", \"ÿÆŸàÿØÿ¥\", \"ÿÆŸàÿØŸÖ\", \"ÿß€å\", \"ŸÖŸÜŸÖ\", \"ÿ™Ÿà€å\", \"⁄ÜŸá\", \"⁄Üÿ±ÿß\", \"⁄©€å\", \"⁄Ü€å\", \"ŸáŸÖ€åŸÜ\",\n",
        "    \"ÿ≠ÿ™€å\", \"ÿßŸÑÿßŸÜ\", \"ÿ≠ÿßŸÑÿß\", \"ÿ™ŸàŸÜÿ≥ÿ™Ÿá\", \"ŸáŸÖŸá\", \"ŸáŸÖŸàŸÜ\", \"⁄©ŸÑ\", \"ŸÇÿ®ŸÑ\", \"Ÿáÿ±⁄©ÿ≥€å\", \"Ÿáÿ±⁄Ü€åÿ≤€å\", \"ŸáŸÖ⁄ÜŸÜ€åŸÜ\", \"ŸáŸÖ⁄ÜŸàŸÜ\", \"ÿ¢€åÿß\",\n",
        "    \"ÿß⁄©ŸÜŸàŸÜ\", \"ÿ®ÿß€åÿØ\", \"ÿ®ÿπÿØ\", \"Ÿæÿ≥\", \"Ÿæ€åÿ¥\", \"ÿ™Ÿàÿ≥ÿ∑\", \"ÿ™ŸÖÿßŸÖ\", \"ÿ™ÿ±\", \"ÿ™ÿ±€åŸÜ\", \"ÿ¨ÿ≤\", \"ÿ≠ÿ™ŸÖÿß\", \"ÿÆ€åŸÑ€å\", \"ÿØ€å⁄Øÿ±\", \"ÿ±ÿßŸá\",\n",
        "    \"ÿ≤€åÿßÿØ\", \"ÿ≥ÿπ€å\", \"ÿ¥ÿØ\", \"ÿ¥ŸÖÿßŸáÿß\", \"⁄©ÿßŸÖŸÑÿß\", \"⁄©ÿßŸÖŸÑ\", \"ŸÖÿ´ŸÑ\", \"ŸÖÿ´ŸÑÿß\", \"ŸÖ€å\", \"ŸáŸÖ⁄Ü€åŸÜ\", \"ŸàÿßŸÇÿπÿß\", \"ŸàŸÑ€å\", \"€å⁄©€å\", \"€åÿπŸÜ€å\",\n",
        "    \"€åŸá\", \"€å⁄©ŸÖ\", \"⁄ÜŸàŸÜ\", \"ÿ®ÿπÿ∂€å\", \"ŸáŸÖŸàÿ∑ŸÜ\", \"Ÿà€å\", \"ÿ¥Ÿà\", \"ÿØŸà\", \"⁄©ÿ±ÿØ\", \"⁄©ÿ±ÿØŸÖ\", \"⁄©ÿ±ÿØŸá\", \"⁄©ÿ±ÿØŸÜ\", \"⁄©ÿ±ÿØŸÜÿØ\", \"⁄©ÿ±ÿØ€å\",\n",
        "    \"⁄©ÿ±ÿØ€åÿØ\", \"⁄©ÿ±ÿØŸá‚ÄåÿßŸÜÿØ\", \"⁄©ÿ±ÿØŸÖ‚Äå\", \"⁄©ŸÜŸÖ\", \"⁄©ŸÜ€å\", \"⁄©ŸÜŸÜÿØ\", \"⁄©ŸÜ€åÿØ\", \"⁄©ŸÜÿØ\", \"⁄©ŸÜ€åŸÖ\", \"⁄©ŸÜÿØŸÖ\", \"⁄©ŸÜ€åŸÖ\", \"⁄©ŸÜŸàŸÜ€å\", \"⁄©ŸÜŸàŸÜ€å‚Äå\",\n",
        "    \"⁄©ÿ±ÿØŸÖ \", \"⁄©ÿ±ÿØŸÖ‚Äå \", \"⁄©ÿ±ÿØ€åŸÖ‚Äå \", \"⁄©ÿ±ÿØŸá‚Äå \", \"⁄©ÿ±ÿØŸá‚ÄåÿßŸÜÿØ \", \"⁄©ÿ±ÿØŸá‚ÄåÿßŸÖ \", \"⁄©ÿ±ÿØŸá‚Äåÿß€å \", \"⁄©ÿ±ÿØŸá‚Äåÿß€åŸÖ \", \"⁄©ÿ±ÿØŸá‚ÄåÿßŸÜÿØ‚Äå \",\n",
        "    \"⁄©ŸÜŸÖ‚Äå \", \"⁄©ŸÜ€åŸÖ‚Äå \", \"⁄©ŸÜ€åŸÖ‚Äå‚Äå \", \"⁄©ŸÜ€åÿØ‚Äå \", \"⁄©ŸÜ€åÿØ‚Äå‚Äå \", \"⁄©ŸÜŸÜÿØ‚Äå \", \"⁄©ÿ±ÿØ‚Äå \", \"⁄©ÿ±ÿØ‚Äå‚Äå \", \"⁄©ÿ±ÿØŸÖ‚Äå‚Äå \", \"⁄©ÿ±ÿØŸá‚Äå‚Äå \",\n",
        "    \"⁄©ÿ±ÿØŸá‚Äå‚Äå‚Äå‚Äå \", \"⁄©ÿ±ÿØŸá‚ÄåÿßŸÖ‚Äå‚Äå \", \"⁄©ÿ±ÿØŸá‚Äåÿß€å‚Äå‚Äå‚Äå \", \"⁄©ÿ±ÿØŸá‚Äåÿß€åŸÖ‚Äå \", \"⁄©ÿ±ÿØŸá‚ÄåÿßŸÜÿØ‚Äå‚Äå \", \"⁄©ŸÜŸÖ‚Äå‚Äå \", \"⁄©ŸÜ€å‚Äå‚Äå \", \"⁄©ŸÜŸÜÿØ‚Äå‚Äå \",\n",
        "    \"⁄©ŸÜ€åÿØ‚Äå‚Äå \", \"⁄©ŸÜ€åÿØ‚Äå‚Äå‚Äå‚Äå \", \"⁄©ŸÜ€åŸÖ‚Äå‚Äå‚Äå‚Äå \", \"⁄©ŸÜŸàŸÜ€å‚Äå‚Äå \", \"⁄©ŸÜŸà\"\n",
        "]\n",
        "# stopwords_count = Counter([word for sublist in df.iloc[:, 2] for word in sublist if word in stopwords])\n",
        "# stopwords_list = sorted(stopwords_count.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# for word, count in stopwords_list:\n",
        "#     print(f\"{word}: {count}\")\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    return filtered_tokens\n",
        "\n",
        "df['data_stopwords_removed'] = df['data_tokenized'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYNB6AkK5C50"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a28zJtUm8Lzl"
      },
      "outputs": [],
      "source": [
        "# lemmatizer = Lemmatizer()\n",
        "# df['data_lemmatized'] = df['data_stopwords_removed'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JScGjdUFTjFn"
      },
      "outputs": [],
      "source": [
        "# df['num_lemmatized_words'] = df.apply(lambda row: sum(1 for i, token in enumerate(row['data_stopwords_removed']) if token != row['data_lemmatized'][i]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgFtycQI6KpR"
      },
      "outputs": [],
      "source": [
        "df['data_preprocessed'] = df['data_tokenized'].apply(lambda tokens: ''.join(tokens))\n",
        "\n",
        "# df['data_preprocessed'] = df['data_stopwords_removed'].apply(lambda tokens: ' '.join(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6pAXLqwetxD"
      },
      "source": [
        "## Word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBMGSfTW5G54"
      },
      "source": [
        "### Padding?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRthP1FucbMb",
        "outputId": "e9a2d0bd-faf3-4934-d50a-83851c9a3f1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "3864    False\n",
            "3865    False\n",
            "3866    False\n",
            "3867    False\n",
            "3868    False\n",
            "Name: data_normalized, Length: 3869, dtype: bool\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "TO RUN THIS CELL\n",
        "RUN THE CELL BELOW\n",
        "BEFORE RUNNING THIS\n",
        "AS MAX_LENGTH COMES\n",
        "THROUGH THE CELL BELOW\n",
        "\n",
        "'''\n",
        "need_padding = df['data_normalized'].apply(lambda tokens: len(tokens) < max_length)\n",
        "print(need_padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6JEndMD8v11"
      },
      "source": [
        "### Finding the max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaRemjRO8Dui",
        "outputId": "51750e60-c56b-43a4-99c7-6e8928dca2e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def max_len(dataset):\n",
        "    return max(dataset['data_tokenized'].apply(lambda x: len(x)))\n",
        "\n",
        "max_length = max_len(df)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3GWsz1DQrfl",
        "outputId": "8496eed8-2947-419f-f5d8-de49cd8e307e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longest max_length as index of data_normalized column: 64\n"
          ]
        }
      ],
      "source": [
        "print('Longest max_length as index of data_normalized column:', df['data_tokenized'].str.len().idxmax())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Atr7Om7tb1"
      },
      "source": [
        "### PARSBERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VjAXYP_r7y76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 434/434 [00:00<00:00, 434kB/s]\n",
            "Downloading (‚Ä¶)solve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.22M/1.22M [00:02<00:00, 527kB/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('HooshvareLab/bert-base-parsbert-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r6SufIOEoYHd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading tf_model.h5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 963M/963M [20:26<00:00, 785kB/s] \n",
            "Some layers from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "bert_model = TFBertModel.from_pretrained('HooshvareLab/bert-base-parsbert-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSowrM6IPPRL",
        "outputId": "3708c23b-c0fa-4685-cdf4-c8f9dbc31027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final embeddings shape: (3869, 68, 768)\n"
          ]
        }
      ],
      "source": [
        "max_length = 68\n",
        "batch_size = 32\n",
        "num_samples = len(df)\n",
        "num_batches = int(np.ceil(num_samples / batch_size))\n",
        "\n",
        "embeddings_list = []\n",
        "\n",
        "for i in range(num_batches):\n",
        "    start_index = i * batch_size\n",
        "    end_index = min((i + 1) * batch_size, num_samples)\n",
        "    batch = df['data_clean'][start_index:end_index]\n",
        "\n",
        "    tokenized_tensors = tokenizer.batch_encode_plus(\n",
        "        batch.tolist(),\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )['input_ids']\n",
        "\n",
        "    outputs = bert_model(tokenized_tensors)\n",
        "    embeddings = outputs.last_hidden_state.numpy()\n",
        "    embeddings_list.append(embeddings)\n",
        "\n",
        "embeddings = np.concatenate(embeddings_list, axis=0)\n",
        "\n",
        "print(f\"Final embeddings shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0rcN9gOPoRP",
        "outputId": "1356ddf8-b1d3-43e7-b1af-fb788a3211c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3869"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_sentences = (241 * batch_size) + 13\n",
        "total_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GfsUlN2t2vZ8"
      },
      "outputs": [],
      "source": [
        "np.save('parsbert_embeddings_input_ids.npy', embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_6uN2mHgK4P"
      },
      "source": [
        "## Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7TDf0JFgNhK"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(df['data_tokenized'],\n",
        "                 vector_size=max_length,\n",
        "                 alpha=0.001,\n",
        "                 window=10,\n",
        "                 min_count=1,\n",
        "                 workers=4,\n",
        "                 sg=1,\n",
        "                 hs=0,\n",
        "                 negative=20\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reewuKeq-8ZA"
      },
      "outputs": [],
      "source": [
        "y = df.iloc[:, 3:14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpbC_u2b8scV"
      },
      "outputs": [],
      "source": [
        "target_vectors = df.iloc[:, 3:14].applymap(lambda x: model.wv[x]).values.tolist()\n",
        "target_vectors_df = pd.DataFrame(target_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fi6bx36p_P_d",
        "outputId": "d128e4a6-9b78-40a4-ef46-3d8daea26ae0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-8a7dd306-b53b-4ac6-8a41-7c814162c689\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3864</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3865</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3866</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3867</th>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3868</th>\n",
              "      <td>[0.35235533, 0.8212422, 0.1741617, 0.5656758, ...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "      <td>[0.35608754, 0.82468987, 0.17659916, 0.5669733...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3869 rows √ó 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a7dd306-b53b-4ac6-8a41-7c814162c689')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-0f1deed6-c888-426d-8dae-7769345e0921\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f1deed6-c888-426d-8dae-7769345e0921')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-0f1deed6-c888-426d-8dae-7769345e0921 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a7dd306-b53b-4ac6-8a41-7c814162c689 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a7dd306-b53b-4ac6-8a41-7c814162c689');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                     0   \\\n",
              "0     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "2     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "4     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "...                                                 ...   \n",
              "3864  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3866  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "\n",
              "                                                     1   \\\n",
              "0     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "2     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "4     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "...                                                 ...   \n",
              "3864  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3866  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3867  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     2   \\\n",
              "0     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "2     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "4     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "...                                                 ...   \n",
              "3864  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3865  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3866  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     3   \\\n",
              "0     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "2     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "4     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "...                                                 ...   \n",
              "3864  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3866  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     4   \\\n",
              "0     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "1     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "2     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "4     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "...                                                 ...   \n",
              "3864  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3866  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     5   \\\n",
              "0     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "2     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "4     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "...                                                 ...   \n",
              "3864  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3866  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     6   \\\n",
              "0     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "1     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "2     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "4     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "...                                                 ...   \n",
              "3864  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3865  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3866  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     7   \\\n",
              "0     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "2     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "4     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "...                                                 ...   \n",
              "3864  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3866  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     8   \\\n",
              "0     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "2     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "4     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "...                                                 ...   \n",
              "3864  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3866  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     9   \\\n",
              "0     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "2     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "4     [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "...                                                 ...   \n",
              "3864  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3866  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...   \n",
              "\n",
              "                                                     10  \n",
              "0     [0.35608754, 0.82468987, 0.17659916, 0.5669733...  \n",
              "1     [0.35608754, 0.82468987, 0.17659916, 0.5669733...  \n",
              "2     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...  \n",
              "3     [0.35608754, 0.82468987, 0.17659916, 0.5669733...  \n",
              "4     [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...  \n",
              "...                                                 ...  \n",
              "3864  [0.35608754, 0.82468987, 0.17659916, 0.5669733...  \n",
              "3865  [0.35608754, 0.82468987, 0.17659916, 0.5669733...  \n",
              "3866  [0.35235533, 0.8212422, 0.1741617, 0.5656758, ...  \n",
              "3867  [0.35608754, 0.82468987, 0.17659916, 0.5669733...  \n",
              "3868  [0.35608754, 0.82468987, 0.17659916, 0.5669733...  \n",
              "\n",
              "[3869 rows x 11 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_vectors_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FElqT41wAIb3",
        "outputId": "515a5d3f-fcc6-4932-8032-8967512cabb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      0   1   2   3   4   5   6  7   8  9   10\n",
            "0      Ÿà  ÿØÿ±  ÿØÿ±   Ÿà  ÿØÿ±   Ÿà  ÿØÿ±  Ÿà   Ÿà  Ÿà   Ÿà\n",
            "1      Ÿà   Ÿà   Ÿà   Ÿà  ÿØÿ±   Ÿà  ÿØÿ±  Ÿà   Ÿà  Ÿà   Ÿà\n",
            "2      Ÿà   Ÿà   Ÿà  ÿØÿ±   Ÿà   Ÿà  ÿØÿ±  Ÿà  ÿØÿ±  Ÿà  ÿØÿ±\n",
            "3      Ÿà   Ÿà  ÿØÿ±   Ÿà   Ÿà   Ÿà  ÿØÿ±  Ÿà  ÿØÿ±  Ÿà   Ÿà\n",
            "4      Ÿà   Ÿà  ÿØÿ±   Ÿà   Ÿà   Ÿà  ÿØÿ±  Ÿà  ÿØÿ±  Ÿà  ÿØÿ±\n",
            "...   ..  ..  ..  ..  ..  ..  .. ..  .. ..  ..\n",
            "3864   Ÿà   Ÿà  ÿØÿ±   Ÿà   Ÿà  ÿØÿ±  ÿØÿ±  Ÿà   Ÿà  Ÿà   Ÿà\n",
            "3865   Ÿà   Ÿà  ÿØÿ±   Ÿà   Ÿà   Ÿà  ÿØÿ±  Ÿà   Ÿà  Ÿà   Ÿà\n",
            "3866   Ÿà   Ÿà  ÿØÿ±   Ÿà   Ÿà   Ÿà  ÿØÿ±  Ÿà  ÿØÿ±  Ÿà  ÿØÿ±\n",
            "3867   Ÿà  ÿØÿ±   Ÿà   Ÿà   Ÿà   Ÿà   Ÿà  Ÿà   Ÿà  Ÿà   Ÿà\n",
            "3868  ÿØÿ±   Ÿà   Ÿà   Ÿà   Ÿà   Ÿà   Ÿà  Ÿà   Ÿà  Ÿà   Ÿà\n",
            "\n",
            "[3869 rows x 11 columns]\n"
          ]
        }
      ],
      "source": [
        "def vector_to_text(vector):\n",
        "    similar_words = model.wv.similar_by_vector(vector, topn=1)\n",
        "    return similar_words[0][0]\n",
        "\n",
        "# Convert the numeric values back to text\n",
        "text_vectors = target_vectors_df.applymap(vector_to_text)\n",
        "\n",
        "# Display the DataFrame with text vectors\n",
        "print(text_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Etc4yIS8dGw",
        "outputId": "70c46030-4fe5-48b2-e7be-3e256a7b1170"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1343379, 1601060)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.train(df['data_tokenized'],\n",
        "               total_examples=len(df['data_tokenized']),\n",
        "               epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HNxz9ZU9UmP",
        "outputId": "3cd9d62f-eb28-4dae-dfa1-7c78c5e4c3fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Ÿàÿ¨ŸàÿØ', 0.9993520379066467),\n",
              " ('ÿ®ÿ±ÿßÿ®ÿ±', 0.9993158578872681),\n",
              " ('ŸÜŸÖ€å\\u200cÿ¥ŸàÿØ', 0.9993118047714233),\n",
              " ('ÿ™ŸàŸÑ€åÿØ', 0.9993026852607727),\n",
              " ('ÿ≠ŸÅÿ∏', 0.99929279088974)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLOv2ZOAd392"
      },
      "outputs": [],
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# words = model.wv.index_to_key\n",
        "# word_vectors = model.wv[words]\n",
        "\n",
        "# tsne = TSNE(n_components=2, random_state=42)\n",
        "# word_vectors_tsne = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for i, word in enumerate(words):\n",
        "#     x, y = word_vectors_tsne[i, :]\n",
        "#     plt.scatter(x, y)\n",
        "#     plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeMA2-yri5Tv"
      },
      "outputs": [],
      "source": [
        "model.save('persian_word2vec.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTJo4169i9nE"
      },
      "outputs": [],
      "source": [
        "word2vec_model = Word2Vec.load('persian_word2vec.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoFxnOx4i_q5"
      },
      "outputs": [],
      "source": [
        "# word_vectors = word2vec_model.wv\n",
        "word_vectors = model.wv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzX2di3rhOla"
      },
      "outputs": [],
      "source": [
        "max_sequence_length = len(df['data_tokenized'][df['data_tokenized'].str.len().idxmax()])\n",
        "\n",
        "embedding_layer = Embedding(input_dim=len(word_vectors.key_to_index),\n",
        "                            output_dim=word_vectors.vector_size,\n",
        "                            weights=[word_vectors.vectors],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extreme learning machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shapes (3095,68,768) and (68,100) not aligned: 768 (dim 2) != 68 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[72], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m biases \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, n_hidden)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Calculate the hidden layer output\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m hidden_output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X_train, input_weights) \u001b[39m+\u001b[39m biases\n\u001b[0;32m     17\u001b[0m hidden_output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(hidden_output, \u001b[39m0\u001b[39m)  \u001b[39m# Apply ReLU activation function\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Solve the output weights using ridge regression\u001b[39;00m\n",
            "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: shapes (3095,68,768) and (68,100) not aligned: 768 (dim 2) != 68 (dim 0)"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(embeddings, df.iloc[:, 3:14], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Define the number of hidden neurons\n",
        "n_hidden = 100\n",
        "\n",
        "# Generate random input weights and biases\n",
        "input_weights = np.random.uniform(-1, 1, (X_train.shape[1], n_hidden))\n",
        "biases = np.random.uniform(-1, 1, n_hidden)\n",
        "\n",
        "# Calculate the hidden layer output\n",
        "hidden_output = np.dot(X_train, input_weights) + biases\n",
        "hidden_output = np.maximum(hidden_output, 0)  # Apply ReLU activation function\n",
        "\n",
        "# Solve the output weights using ridge regression\n",
        "output_weights = np.dot(np.linalg.pinv(hidden_output), y_train)\n",
        "\n",
        "# Calculate the testing hidden layer output\n",
        "hidden_output_test = np.dot(X_test, input_weights) + biases\n",
        "hidden_output_test = np.maximum(hidden_output_test, 0)  # Apply ReLU activation function\n",
        "\n",
        "# Predict the testing labels\n",
        "y_pred = np.dot(hidden_output_test, output_weights)\n",
        "y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True) \n",
        "\n",
        "# Convert the predictions to class labels\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPv5J0FchGlU"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3tew2RSTkfZ8"
      },
      "outputs": [],
      "source": [
        "X = embeddings\n",
        "y = df.iloc[:, 3:14]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAJklN6zoHzH"
      },
      "outputs": [],
      "source": [
        "# class_distribution = np.sum(y_train, axis=0) / y_train.shape[0]\n",
        "\n",
        "# class_weights = 1 / (class_distribution + 1e-6)\n",
        "# class_weights /= np.sum(class_weights)\n",
        "\n",
        "# class_weights_dict = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERsd1vOEqutk"
      },
      "outputs": [],
      "source": [
        "# small_details_weight = 2.0\n",
        "\n",
        "# model.add(LSTM(units=128, return_sequences=True))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(LSTM(units=64, return_sequences=True))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(LSTM(units=32))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Dense(11, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "# # Use Focal Loss as the loss function\n",
        "# def focal_loss(gamma=2.0, alpha=0.25):\n",
        "#     def focal_loss_fn(y_true, y_pred):\n",
        "#         ce_loss = CategoricalCrossentropy()(y_true, y_pred)\n",
        "#         pt = K.exp(-ce_loss)\n",
        "#         focal_loss = alpha * K.pow(1 - pt, gamma) * ce_loss\n",
        "#         return focal_loss\n",
        "#     return focal_loss_fn\n",
        "\n",
        "# model.compile(loss=focal_loss(), optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
        "\n",
        "# loss, accuracy = model.evaluate(X_test, y_test)\n",
        "# print('Test loss:', loss)\n",
        "# print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "EoMGHDaUka2_"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(\n",
        "              units=32,\n",
        "              activation='tanh',\n",
        "              recurrent_activation='hard_sigmoid',\n",
        "              return_sequences=True\n",
        "              ))\n",
        "model.add(LayerNormalization(\n",
        "                            axis=-1,\n",
        "                            epsilon=0.001,\n",
        "                            center=True,\n",
        "                            scale=True,\n",
        "                            beta_initializer='zeros',\n",
        "                            gamma_initializer='ones',\n",
        "                            beta_regularizer=None,\n",
        "                            gamma_regularizer=None,\n",
        "                            beta_constraint=None,\n",
        "                            gamma_constraint=None\n",
        "                            ))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(LSTM(units=64, return_sequences=True))\n",
        "model.add(LSTM(units=24))\n",
        "\n",
        "model.add(Dense(11, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy']) #sample_weight_mode=class_weights_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, epochs=100, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC89gslNo_1z",
        "outputId": "6b9ffa2e-d86f-45e9-8379-d59437420f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 9s 356ms/step\n",
            "Precision: 0.4691358024691358\n",
            "Recall: 0.023543990086741014\n",
            "F-score: 0.04483775811209439\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "threshold = 0.5\n",
        "y_test_threshold = (y_test > threshold).astype(int)\n",
        "y_pred_threshold = (y_pred > threshold).astype(int)\n",
        "\n",
        "precision, recall, f_score, _ = precision_recall_fscore_support(y_test_threshold, y_pred_threshold, average='micro')\n",
        "\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F-score:', f_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1N9x8t341wH",
        "outputId": "b810ca16-1584-49ef-c1b8-00778cc5d82a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.00      0.00       189\n",
            "           1       1.00      0.00      0.00       111\n",
            "           2       1.00      0.00      0.00       397\n",
            "           3       0.00      0.00      1.00        65\n",
            "           4       0.00      0.00      1.00        50\n",
            "           5       0.00      0.00      1.00        84\n",
            "           6       0.57      0.58      0.58       391\n",
            "           7       0.00      0.00      1.00        26\n",
            "           8       0.12      0.04      0.05       141\n",
            "           9       1.00      0.00      0.00        25\n",
            "          10       1.00      0.00      0.00       135\n",
            "\n",
            "   micro avg       0.51      0.14      0.22      1614\n",
            "   macro avg       0.52      0.06      0.42      1614\n",
            "weighted avg       0.68      0.14      0.28      1614\n",
            " samples avg       0.72      0.12      0.45      1614\n",
            "\n"
          ]
        }
      ],
      "source": [
        "report = classification_report(y_test_threshold, y_pred_threshold, zero_division=1)\n",
        "print(report)\n",
        "# with open('classification_report.txt', \"w\") as file:\n",
        "#     file.write(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "owOcxQ4O7q7H",
        "outputId": "7ff5eeeb-99aa-4254-f341-c2c539e1f13d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTMklEQVR4nO3de3zP9f//8ft7Z2Yzmm0OY4QcmtPEZySVZUl89FGEkEUyCy3lfCiyqJxyynF8whyKDxFfyZRTchhzZmiO08hptLG9fn/0865l0zZ7eW9zu14ur8tl79fr+Xq9Hu/XZnN/P5+v58tiGIYhAAAAAACQ6+xsXQAAAAAAAAUVoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwCAfCAyMlIWi0UnT558IOd7+umn9fjjjz+Qc2XV8OHDZbFYbF0GAADZQugGAOAv7oRbi8WiTZs23bXdMAz5+vrKYrHoxRdfzNE5pkyZosjIyPustOD4/fffNW7cONWvX19FixaVi4uLKleurLCwMB05csTW5QEAcF8I3QAAZMDFxUULFiy4a/3GjRt1+vRpOTs75/jYOQndHTt21M2bN1WuXLkcnzcvSkxM1JNPPqnw8HB5eXnpww8/1OTJk9WqVSutWLEiz/W2AwCQXQ62LgAAgLzohRde0JIlSzRx4kQ5OPz553LBggUKCAhQYmLiA6kjKSlJrq6usre3l729/QM554P0+uuva/fu3Vq6dKlat26dbtuIESM0aNAgG1UGAEDuoKcbAIAMtGvXThcvXtS6deus61JSUrR06VK1b98+w33S0tI0fvx4Va9eXS4uLvL29lb37t3122+/Wdv4+flp//792rhxo3UY+9NPPy3pz6HtGzduVGhoqLy8vFSmTJl02/5+T/e3336rxo0by83NTe7u7nriiSfS9dAfPXpUrVu3lo+Pj1xcXFSmTBm9+uqrunLlSpauw86dO9WgQQMVKlRI5cuX17Rp06zbrl+/LldXV/Xu3fuu/U6fPi17e3tFRERkeuyffvpJq1at0htvvHFX4JYkZ2dnffrpp/esb86cOXr22Wfl5eUlZ2dnVatWTVOnTr2r3Y4dOxQcHCxPT0/rewkJCUnXJioqSgEBAdZr6e/vrwkTJtzz/AAA/BN6ugEAyICfn58CAwO1cOFCNWvWTNIfAffKlSt69dVXNXHixLv26d69uyIjI9WlSxf16tVLJ06c0KRJk7R7925t3rxZjo6OGj9+vN5++20VKVLE2ovr7e2d7jihoaEqUaKEhg4dqqSkpExrjIyMVEhIiKpXr64BAwbIw8NDu3fv1po1a9S+fXulpKQoODhYycnJevvtt+Xj46MzZ87om2++0eXLl1W0aNF7XoPffvtNL7zwgtq0aaN27dpp8eLF6tGjh5ycnBQSEqIiRYropZde0qJFizR27Nh0PfELFy6UYRjq0KFDpsdfsWKFpD+GzufU1KlTVb16dbVs2VIODg5auXKlQkNDlZaWpp49e0qSLly4oKZNm6pEiRLq37+/PDw8dPLkSX399dfW46xbt07t2rVTkyZNNHr0aEnSwYMHtXnz5gw/VAAAIMsMAABgNWfOHEOS8fPPPxuTJk0y3NzcjBs3bhiGYRivvPKK8cwzzxiGYRjlypUzmjdvbt3vxx9/NCQZ8+fPT3e8NWvW3LW+evXqRuPGjTM995NPPmncvn07w20nTpwwDMMwLl++bLi5uRn169c3bt68ma5tWlqaYRiGsXv3bkOSsWTJkmxfh8aNGxuSjM8++8y6Ljk52ahVq5bh5eVlpKSkGIZhGGvXrjUkGd9++226/WvUqJHhe/yrl156yZBk/Pbbb1mqadiwYcbf/+ty53vzV8HBwUaFChWsr5ctW2b9nmamd+/ehru7+13XHQCA+8XwcgAAMtGmTRvdvHlT33zzja5du6Zvvvkm06HlS5YsUdGiRfXcc88pMTHRugQEBKhIkSLasGFDls/brVu3f7x/e926dbp27Zr69+8vFxeXdNvuPFbrTk/22rVrdePGjSyf/w4HBwd1797d+trJyUndu3fXhQsXtHPnTklSUFCQSpUqpfnz51vb7du3T3v37tVrr712z+NfvXpVkuTm5pbt2u4oVKiQ9esrV64oMTFRjRs31vHjx61D6D08PCRJ33zzjW7dupXhcTw8PJSUlJTudgIAAHIDoRsAgEyUKFFCQUFBWrBggb7++mulpqbq5ZdfzrDt0aNHdeXKFXl5ealEiRLpluvXr+vChQtZPm/58uX/sU1cXJwk3XN27/Llyys8PFwzZ86Up6engoODNXny5Czfz12qVCm5urqmW1e5cmVJst5bbmdnpw4dOmj58uXWYD9//ny5uLjolVdeuefx3d3dJUnXrl3LUj0Z2bx5s4KCguTq6ioPDw+VKFFCAwcOlCTr+2zcuLFat26tDz74QJ6envr3v/+tOXPmKDk52Xqc0NBQVa5cWc2aNVOZMmUUEhKiNWvW5LguAADuIHQDAHAP7du317fffqtp06apWbNm1l7Tv0tLS5OXl5fWrVuX4fLhhx9m+Zx/7b29X5999pn27t2rgQMH6ubNm+rVq5eqV6+u06dP59o5OnXqpOvXr2v58uUyDEMLFizQiy+++I/3jFepUkWSFBsbm6PzxsXFqUmTJkpMTNTYsWO1atUqrVu3Tu+8846kP74n0h89/0uXLtXWrVsVFhamM2fOKCQkRAEBAbp+/bokycvLSzExMVqxYoVatmypDRs2qFmzZurcuXOOagMA4A5CNwAA9/DSSy/Jzs5O27Zty3RouSQ9+uijunjxoho2bKigoKC7lpo1a1rb3hn+fT8effRRSX8M5f4n/v7+Gjx4sH744Qf9+OOPOnPmTLpZyDNz9uzZuyZyO3LkiKQ/Jpq74/HHH1ft2rU1f/58/fjjj4qPj8/S5GgtWrSQJH355Zf/2DYjK1euVHJyslasWKHu3bvrhRdeUFBQUKYfWvzrX//SRx99pB07dmj+/Pnav3+/oqKirNudnJzUokULTZkyRXFxcerevbvmzZunY8eO5ag+AAAkQjcAAPdUpEgRTZ06VcOHD7eGxIy0adNGqampGjFixF3bbt++rcuXL1tfu7q6pnudE02bNpWbm5siIiL0+++/p9tmGIakP+6Zvn37drpt/v7+srOzSze0OjO3b9/WF198YX2dkpKiL774QiVKlFBAQEC6th07dtT//d//afz48XrkkUesM77fS2BgoJ5//nnNnDlTy5cvv2t7SkqK+vbtm+n+d+57v/N+pT+GlM+ZMyddu99++y1dG0mqVauWJFmvw8WLF9Ntt7OzU40aNdK1AQAgJ3hkGAAA/yArQ4wbN26s7t27KyIiQjExMWratKkcHR119OhRLVmyRBMmTLDeDx4QEKCpU6dq5MiRqlixory8vPTss89mqyZ3d3eNGzdOXbt21RNPPKH27durWLFi2rNnj27cuKG5c+fq+++/V1hYmF555RVVrlxZt2/f1n//+1/Z29tn+FzsvytVqpRGjx6tkydPqnLlylq0aJFiYmI0ffp0OTo6pmvbvn17vf/++1q2bJl69Ohx1/bMzJs3T02bNtV//vMftWjRQk2aNJGrq6uOHj2qqKgonTt3LtNndTdt2tTaO929e3ddv35dM2bMkJeXl86dO2dtN3fuXE2ZMkUvvfSSHn30UV27dk0zZsyQu7u7XnjhBUlS165ddenSJT377LMqU6aMfvnlF33++eeqVauWqlatmqX3AgBARgjdAADkkmnTpikgIEBffPGFBg4cKAcHB/n5+em1115Tw4YNre2GDh2qX375RWPGjNG1a9fUuHHjbIduSXrjjTfk5eWljz/+WCNGjJCjo6OqVKlivae5Zs2aCg4O1sqVK3XmzBkVLlxYNWvW1Lfffqt//etf/3j8YsWKae7cuXr77bc1Y8YMeXt7a9KkSerWrdtdbb29vdW0aVOtXr06W8/dLlGihLZs2aIpU6Zo0aJFGjRokFJSUlSuXDm1bNnyns/Ifuyxx7R06VINHjxYffv2lY+Pj3r06KESJUooJCTE2q5x48bavn27oqKilJCQoKJFi6pevXqaP3++ddK61157TdOnT9eUKVN0+fJl+fj4qG3btho+fLjs7BgYCADIOYvx9/FWAAAAOfDSSy8pNjaWe6ABAPgLProFAAD37dy5c1q1alW2erkBAHgYMLwcAADk2IkTJ7R582bNnDlTjo6O6t69u61LAgAgT6GnGwAA5NjGjRvVsWNHnThxQnPnzpWPj4+tSwIAIE/hnm4AAAAAAExCTzcAAAAAACYhdAMAAAAAYJKHbiK1tLQ0nT17Vm5ubrJYLLYuBwAAAACQDxmGoWvXrqlUqVKys8u8P/uhC91nz56Vr6+vrcsAAAAAABQAp06dUpkyZTLd/tCFbjc3N0l/XBh3d3cbVwMAAAAAyI+uXr0qX19fa8bMzEMXuu8MKXd3dyd0AwAAAADuyz/dtsxEagAAAAAAmITQDQAAAACASQjdAAAAAACY5KG7pzurUlNTdevWLVuXgWxwdHSUvb29rcsAAAAAACtC998YhqHz58/r8uXLti4FOeDh4SEfHx+ewQ4AAAAgTyB0/82dwO3l5aXChQsT3vIJwzB048YNXbhwQZJUsmRJG1cEAAAAAITudFJTU62B+5FHHrF1OcimQoUKSZIuXLggLy8vhpoDAAAAsDkmUvuLO/dwFy5c2MaVIKfufO+4Hx8AAABAXkDozgBDyvMvvncAAAAA8hJCNwAAAAAAJiF0I8eio6NlsViyNNN7dtoCAAAAQEHBRGpZ5Nd/1QM938mPmz/Q8+VEgwYNdO7cORUtWjRX2wIAAABAQUFP90MqJSXlvo/h5OSU5WdiZ6ctAAAAABQUhO4C4umnn1ZYWJjCwsJUtGhReXp6asiQITIMQ5Lk5+enESNGqFOnTnJ3d9ebb74pSdq0aZMaNWqkQoUKydfXV7169VJSUpL1uMnJyerXr598fX3l7OysihUratasWZLuHjL+yy+/qEWLFipWrJhcXV1VvXp1rV69OsO2kvTVV1+pevXqcnZ2lp+fnz777LN078nPz0+jRo1SSEiI3NzcVLZsWU2fPt2sSwgAAAAAuY7QXYDMnTtXDg4O2r59uyZMmKCxY8dq5syZ1u2ffvqpatasqd27d2vIkCGKi4vT888/r9atW2vv3r1atGiRNm3apLCwMOs+nTp10sKFCzVx4kQdPHhQX3zxhYoUKZLh+Xv27Knk5GT98MMPio2N1ejRozNtu3PnTrVp00avvvqqYmNjNXz4cA0ZMkSRkZHp2n322WeqW7eudu/erdDQUPXo0UOHDx++/4sFAAAAAA8A93QXIL6+vho3bpwsFosee+wxxcbGaty4cerWrZsk6dlnn9W7775rbd+1a1d16NBBffr0kSRVqlRJEydOVOPGjTV16lTFx8dr8eLFWrdunYKCgiRJFSpUyPT88fHxat26tfz9/f+x7dixY9WkSRMNGTJEklS5cmUdOHBAn3zyiV5//XVruxdeeEGhoaGSpH79+mncuHHasGGDHnvssexfIAAAAAB4wGza0/3DDz+oRYsWKlWqlCwWi5YvX/6P+0RHR6tOnTrWoc5/7xl9mP3rX/9Kd890YGCgjh49qtTUVElS3bp107Xfs2ePIiMjVaRIEesSHBystLQ0nThxQjExMbK3t1fjxo2zdP5evXpp5MiRatiwoYYNG6a9e/dm2vbgwYNq2LBhunUNGzZMV68k1ahRw/q1xWKRj4+PLly4kKV6AAAAAMDWbBq6k5KSVLNmTU2ePDlL7U+cOKHmzZvrmWeeUUxMjPr06aOuXbtq7dq1JldaMLi6uqZ7ff36dXXv3l0xMTHWZc+ePTp69KgeffRRFSpUKFvH79q1q44fP66OHTsqNjZWdevW1eeff35fNTs6OqZ7bbFYlJaWdl/HBAAAAIAHxabDy5s1a6ZmzZpluf20adNUvnx564RbVatW1aZNmzRu3DgFBwebVWa+8dNPP6V7vW3bNlWqVEn29vYZtq9Tp44OHDigihUrZrjd399faWlp2rhxo3V4+T/x9fXVW2+9pbfeeksDBgzQjBkz9Pbbb9/VrmrVqtq8eXO6dZs3b1blypUzrRcAAAAA8pt8NZHa1q1b7wp/wcHB2rp1q40qylvi4+MVHh6uw4cPa+HChfr888/Vu3fvTNv369dPW7ZsUVhYmGJiYnT06FH973//s06k5ufnp86dOyskJETLly/XiRMnFB0drcWLF2d4vD59+mjt2rU6ceKEdu3apQ0bNqhq1aoZtn333Xe1fv16jRgxQkeOHNHcuXM1adIk9e3b9/4vBAAAAADkEflqIrXz58/L29s73Tpvb29dvXpVN2/ezHA4dHJyspKTk62vr169anqdttKpUyfdvHlT9erVk729vXr37m19NFhGatSooY0bN2rQoEFq1KiRDMPQo48+qrZt21rbTJ06VQMHDlRoaKguXryosmXLauDAgRkeLzU1VT179tTp06fl7u6u559/XuPGjcuwbZ06dbR48WINHTpUI0aMUMmSJfXhhx+mm0QNAAAAAPI7i3HnQc42ZrFYtGzZMrVq1SrTNpUrV1aXLl00YMAA67rVq1erefPmunHjRoahe/jw4frggw/uWn/lyhW5u7unW/f777/rxIkTKl++vFxcXHL+Zmzg6aefVq1atTR+/Hhbl2JT+fl7iPzNr/+q+9r/5MfNc6mS/IdrZ0PDi97n/ldypw7kK/fzb5Z/r8gJ/k4gr7p69aqKFi2aYbb8q3w1vNzHx0cJCQnp1iUkJMjd3T3TSb8GDBigK1euWJdTp049iFIBAAAAAMhfw8sDAwO1evXqdOvWrVunwMDATPdxdnaWs7Oz2aUBkOg1AwAAAP7GpqH7+vXrOnbsmPX1nWdDFy9eXGXLltWAAQN05swZzZs3T5L01ltvadKkSXr//fcVEhKi77//XosXL9aqVfc35KQgiI6OtnUJAAA8PO7nQ0Y+YASQX/C7LlfYNHTv2LFDzzzzjPV1eHi4JKlz586KjIzUuXPnFB8fb91evnx5rVq1Su+8844mTJigMmXKaObMmTwuDLmPXzAAAAAAcoFNQ/fTTz+te83jFhkZmeE+u3fvNrEqAAAAAAByR76aSA0AAAAAgPyE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdyLHhw4erVq1a1tevv/66WrVqZbN6AAAAACCvsens5fnK/TxCKkfn47FTAAAAAJDf0dNdQKWkpNi6BAAAAAB46BG6C4inn35aYWFh6tOnjzw9PRUcHKx9+/apWbNmKlKkiLy9vdWxY0clJiZa90lLS9OYMWNUsWJFOTs7q2zZsvroo4+s2/v166fKlSurcOHCqlChgoYMGaJbt27Z4u0BAAAAQL5E6C5A5s6dKycnJ23evFkff/yxnn32WdWuXVs7duzQmjVrlJCQoDZt2ljbDxgwQB9//LGGDBmiAwcOaMGCBfL29rZud3NzU2RkpA4cOKAJEyZoxowZGjdunC3eGgAAAADkS9zTXYBUqlRJY8aMkSSNHDlStWvX1qhRo6zbZ8+eLV9fXx05ckQlS5bUhAkTNGnSJHXu3FmS9Oijj+rJJ5+0th88eLD1az8/P/Xt21dRUVF6//33H9A7AgAAAID8jdBdgAQEBFi/3rNnjzZs2KAiRYrc1S4uLk6XL19WcnKymjRpkunxFi1apIkTJyouLk7Xr1/X7du35e7ubkrtAAAAAFAQEboLEFdXV+vX169fV4sWLTR69Oi72pUsWVLHjx+/57G2bt2qDh066IMPPlBwcLCKFi2qqKgoffbZZ7leNwAAAAAUVITuAqpOnTr66quv5OfnJweHu7/NlSpVUqFChbR+/Xp17dr1ru1btmxRuXLlNGjQIOu6X375xdSaAQAAAKCgYSK1Aqpnz566dOmS2rVrp59//llxcXFau3atunTpotTUVLm4uKhfv356//33NW/ePMXFxWnbtm2aNWuWpD9CeXx8vKKiohQXF6eJEydq2bJlNn5XAAAAAJC/ELoLqFKlSmnz5s1KTU1V06ZN5e/vrz59+sjDw0N2dn9824cMGaJ3331XQ4cOVdWqVdW2bVtduHBBktSyZUu98847CgsLU61atbRlyxYNGTLElm8JAAAAAPIdhpdn1fArtq7gnqKjo+9aV6lSJX399deZ7mNnZ6dBgwalG0L+V2PGjLHOhn5Hnz59rF8PHz5cw4cPt76OjIzMTskAAAAAUODR0w0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJmL0cAAAAyOuGF73P/fP2k3iAgoyebgAAAAAATELoBgAAAADAJIRuAAAAAABMQuguIAzD0JtvvqnixYvLYrEoJibG1iUBAAAAwEOPidSyyH+u/wM9X2zn2Gy1X7NmjSIjIxUdHa0KFSroyJEjatGihXbu3Klz585p2bJlatWqlTnFAgAAAAAyRE93AREXF6eSJUuqQYMG8vHxUVJSkmrWrKnJkyfburRMpaSk2LoEAAAAADAVobsAeP311/X2228rPj5eFotFfn5+atasmUaOHKmXXnopy8cxDEPDhw9X2bJl5ezsrFKlSqlXr17W7cnJyerXr598fX3l7OysihUratasWdbtGzduVL169eTs7KySJUuqf//+un37tnX7008/rbCwMPXp00eenp4KDg6WJO3bt0/NmjVTkSJF5O3trY4dOyoxMTEXrgwAAAAA2BahuwCYMGGCPvzwQ5UpU0bnzp3Tzz//nKPjfPXVVxo3bpy++OILHT16VMuXL5e//5/D6jt16qSFCxdq4sSJOnjwoL744gsVKVJEknTmzBm98MILeuKJJ7Rnzx5NnTpVs2bN0siRI9OdY+7cuXJyctLmzZs1bdo0Xb58Wc8++6xq166tHTt2aM2aNUpISFCbNm1yfkEAAAAAII/gnu4CoGjRonJzc5O9vb18fHxyfJz4+Hj5+PgoKChIjo6OKlu2rOrVqydJOnLkiBYvXqx169YpKChIklShQgXrvlOmTJGvr68mTZoki8WiKlWq6OzZs+rXr5+GDh0qO7s/Pt+pVKmSxowZY91v5MiRql27tkaNGmVdN3v2bPn6+urIkSOqXLlyjt8PAAAAANgaPd0PqVGjRqlIkSLWJT4+Xq+88opu3rypChUqqFu3blq2bJl1eHhMTIzs7e3VuHHjDI938OBBBQYGymKxWNc1bNhQ169f1+nTp63rAgIC0u23Z88ebdiwIV0tVapUkfTHfeoAAAAAkJ/R0/2Qeuutt9IN4S5VqpQcHBx0+PBhfffdd1q3bp1CQ0P1ySefaOPGjSpUqFCunNfV1TXd6+vXr6tFixYaPXr0XW1LliyZK+cEAAAAAFshdD+kihcvruLFi9+1vlChQmrRooVatGihnj17qkqVKoqNjZW/v7/S0tK0ceNG6/Dyv6pataq++uorGYZh7e3evHmz3NzcVKZMmUzrqFOnjr766iv5+fnJwYEfRwAAAAAFC8PLC6jr168rJiZGMTExkqQTJ04oJiZG8fHxme4TGRmpWbNmad++fTp+/Li+/PJLFSpUSOXKlZOfn586d+6skJAQLV++XCdOnFB0dLQWL14sSQoNDdWpU6f09ttv69ChQ/rf//6nYcOGKTw83Ho/d0Z69uypS5cuqV27dvr5558VFxentWvXqkuXLkpNTc3VawIAAAAADxqhu4DasWOHateurdq1a0uSwsPDVbt2bQ0dOjTTfTw8PDRjxgw1bNhQNWrU0HfffaeVK1fqkUcekSRNnTpVL7/8skJDQ1WlShV169ZNSUlJkqTSpUtr9erV2r59u2rWrKm33npLb7zxhgYPHnzPOkuVKqXNmzcrNTVVTZs2lb+/v/r06SMPD497hnUAAAAAyA8Yz5tFsZ1jbV3CPfXp00d9+vSxvn766adlGEa2jtGqVSu1atUq0+0uLi4aO3asxo4dm+H2xo0ba/v27ZnuHx0dneH6SpUq6euvv85OqQAAAACQL9CVCAAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0J2B7E5AhryD7x0AAACAvITQ/ReOjo6SpBs3bti4EuTUne/dne8lAAAAANgSjwz7C3t7e3l4eOjChQuSpMKFC8tisdi4KmSFYRi6ceOGLly4IA8PD9nb29u6JAAAAAAgdP+dj4+PJFmDN/IXDw8P6/cQAAAAAGyN0P03FotFJUuWlJeXl27dumXrcpANjo6O9HADAAAAyFMI3Zmwt7cnwAEAAAAA7gsTqQEAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBKbh+7JkyfLz89PLi4uql+/vrZv337P9uPHj9djjz2mQoUKydfXV++8845+//33B1QtAAAAAABZZ9PQvWjRIoWHh2vYsGHatWuXatasqeDgYF24cCHD9gsWLFD//v01bNgwHTx4ULNmzdKiRYs0cODAB1w5AAAAAAD/zKahe+zYserWrZu6dOmiatWqadq0aSpcuLBmz56dYfstW7aoYcOGat++vfz8/NS0aVO1a9fuH3vHAQAAAACwBZuF7pSUFO3cuVNBQUF/FmNnp6CgIG3dujXDfRo0aKCdO3daQ/bx48e1evVqvfDCC5meJzk5WVevXk23AAAAAADwIDjY6sSJiYlKTU2Vt7d3uvXe3t46dOhQhvu0b99eiYmJevLJJ2UYhm7fvq233nrrnsPLIyIi9MEHH+Rq7QAAAAAAZIXNJ1LLjujoaI0aNUpTpkzRrl279PXXX2vVqlUaMWJEpvsMGDBAV65csS6nTp16gBUDAAAAAB5mNuvp9vT0lL29vRISEtKtT0hIkI+PT4b7DBkyRB07dlTXrl0lSf7+/kpKStKbb76pQYMGyc7u7s8QnJ2d5ezsnPtvAAAAAACAf2Cznm4nJycFBARo/fr11nVpaWlav369AgMDM9znxo0bdwVre3t7SZJhGOYVCwAAAABADtisp1uSwsPD1blzZ9WtW1f16tXT+PHjlZSUpC5dukiSOnXqpNKlSysiIkKS1KJFC40dO1a1a9dW/fr1dezYMQ0ZMkQtWrSwhm8AAAAAAPIKm4butm3b6tdff9XQoUN1/vx51apVS2vWrLFOrhYfH5+uZ3vw4MGyWCwaPHiwzpw5oxIlSqhFixb66KOPbPUWAAAAAADIlE1DtySFhYUpLCwsw23R0dHpXjs4OGjYsGEaNmzYA6gMAAAAAID7k69mLwcAAAAAID8hdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmcbB1AcicX/9V97X/yY+b51IlAAAAAICcoKcbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMImDrQsAAAB4mPjP9b+v/WM7x+ZSJQCAB4GebgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAEzC7OXI1P3MrsrMqsgJZvQFAABAQUNPNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEgdbFwATDS96f/uXL5s7dQAAAADAQ4qebgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATHJfofv333/PrToAAAAAAChwsh2609LSNGLECJUuXVpFihTR8ePHJUlDhgzRrFmzcr1AAAAAAADyq2yH7pEjRyoyMlJjxoyRk5OTdf3jjz+umTNnZruAyZMny8/PTy4uLqpfv762b99+z/aXL19Wz549VbJkSTk7O6ty5cpavXp1ts8LAAAAAIDZsh26582bp+nTp6tDhw6yt7e3rq9Zs6YOHTqUrWMtWrRI4eHhGjZsmHbt2qWaNWsqODhYFy5cyLB9SkqKnnvuOZ08eVJLly7V4cOHNWPGDJUuXTq7bwMAAAAAANM5ZHeHM2fOqGLFinetT0tL061bt7J1rLFjx6pbt27q0qWLJGnatGlatWqVZs+erf79+9/Vfvbs2bp06ZK2bNkiR0dHSZKfn1923wIAAAAAAA9Etnu6q1Wrph9//PGu9UuXLlXt2rWzfJyUlBTt3LlTQUFBfxZjZ6egoCBt3bo1w31WrFihwMBA9ezZU97e3nr88cc1atQopaamZvdtAAAAAABgumz3dA8dOlSdO3fWmTNnlJaWpq+//lqHDx/WvHnz9M0332T5OImJiUpNTZW3t3e69d7e3pkOUz9+/Li+//57dejQQatXr9axY8cUGhqqW7duadiwYRnuk5ycrOTkZOvrq1evZrlGAAAAAADuR7Z7uv/9739r5cqV+u677+Tq6qqhQ4fq4MGDWrlypZ577jkzarRKS0uTl5eXpk+froCAALVt21aDBg3StGnTMt0nIiJCRYsWtS6+vr6m1ggAAAAAwB3Z7umWpEaNGmndunX3dWJPT0/Z29srISEh3fqEhAT5+PhkuE/JkiXl6OiYbgK3qlWr6vz580pJSUk3m/odAwYMUHh4uPX11atXCd4AAAAAgAci2z3ducXJyUkBAQFav369dV1aWprWr1+vwMDADPdp2LChjh07prS0NOu6I0eOqGTJkhkGbklydnaWu7t7ugUAAAAAgAch26Hbzs5O9vb2mS7ZER4erhkzZmju3Lk6ePCgevTooaSkJOts5p06ddKAAQOs7Xv06KFLly6pd+/eOnLkiFatWqVRo0apZ8+e2X0bAAAAAACYLtvDy5ctW5bu9a1bt7R7927NnTtXH3zwQbaO1bZtW/36668aOnSozp8/r1q1amnNmjXWydXi4+NlZ/fn5wK+vr5au3at3nnnHdWoUUOlS5dW79691a9fv+y+DQAAAAAATJft0P3vf//7rnUvv/yyqlevrkWLFumNN97I1vHCwsIUFhaW4bbo6Oi71gUGBmrbtm3ZOgcAAAAAALaQa/d0/+tf/0p3fzYAAAAAAA+7XAndN2/e1MSJE1W6dOncOBwAAAAAAAVCtoeXFytWTBaLxfraMAxdu3ZNhQsX1pdffpmrxQEAAAAAkJ9lO3SPGzcuXei2s7NTiRIlVL9+fRUrVixXiwMAAAAAID/Lduh+/fXXTSgDAAAAAICCJ0uhe+/evVk+YI0aNXJcDAAAAAAABUmWQnetWrVksVhkGMY921ksFqWmpuZKYQAAAAAA5HdZCt0nTpwwuw4AAIAHxq//qvva/6RLLhUCACjwshS6y5UrZ3YdAAAAAAAUONmeSO2OAwcOKD4+XikpKenWt2zZ8r6LAgAAAACgIMh26D5+/LheeuklxcbGprvP+85jxLinGwAAAACAP9hld4fevXurfPnyunDhggoXLqz9+/frhx9+UN26dRUdHW1CiQAAAAAA5E/Z7uneunWrvv/+e3l6esrOzk52dnZ68sknFRERoV69emn37t1m1AkAAAAAQL6T7Z7u1NRUubm5SZI8PT119uxZSX9Mtnb48OHcrQ4AAAAAgHws2z3djz/+uPbs2aPy5curfv36GjNmjJycnDR9+nRVqFDBjBoBPED38xgdHqEDAAAApJft0D148GAlJSVJkj788EO9+OKLatSokR555BEtWrQo1wsEAAAAACC/ynLorlu3rrp27ar27dvL3d1dklSxYkUdOnRIly5dUrFixawzmAMAAAAAgGzc012zZk29//77KlmypDp16pRupvLixYsTuAEAAAAA+Jssh+5Zs2bp/Pnzmjx5suLj49WkSRNVrFhRo0aN0pkzZ8ysEQAAAACAfClbs5cXLlxYr7/+uqKjo3XkyBG9+uqr+uKLL+Tn56fmzZvr66+/NqtOAAAAAADynWw/MuyORx99VCNHjtTJkye1cOFCbdu2Ta+88kpu1gYAAAAAQL6W7dnL/yo6Olpz5szRV199JQcHB3Xr1i236gIAAAAAIN/Ldug+ffq0IiMjFRkZqePHj6tRo0aaMmWKXnnlFRUqVMiMGgEAAAAAyJeyHLoXL16s2bNna/369fLy8lLnzp0VEhKiihUrmlkfAAAAAAD5VpZD92uvvabmzZtr2bJleuGFF2Rnl+PbwQEAAAAAeChkOXSfPn1aXl5eZtYCAAAAAECBkuXuagI3AAAAAADZwxhxAAAAAABMQugGAAAAAMAkhG4AAAAAAEyS7dD9888/66effrpr/U8//aQdO3bkSlEAAAAAABQE2Q7dPXv21KlTp+5af+bMGfXs2TNXigIAAAAAoCDIdug+cOCA6tSpc9f62rVr68CBA7lSFAAAAAAABUG2Q7ezs7MSEhLuWn/u3Dk5OGT5sd8AAAAAABR42Q7dTZs21YABA3TlyhXrusuXL2vgwIF67rnncrU4AAAAAADys2x3TX/66ad66qmnVK5cOdWuXVuSFBMTI29vb/33v//N9QIBAAAAAMivsh26S5curb1792r+/Pnas2ePChUqpC5duqhdu3ZydHQ0o0YAAAAAAPKlHN2E7erqqjfffDO3awEAAAAAoEDJUuhesWKFmjVrJkdHR61YseKebVu2bJkrhQEAAAAAkN9lKXS3atVK58+fl5eXl1q1apVpO4vFotTU1NyqDQAAAACAfC1LoTstLS3DrwEAAAAAQOay9ciwW7duqUmTJjp69KhZ9QAAAAAAUGBkK3Q7Ojpq7969ZtUCAAAAAECBkq3QLUmvvfaaZs2aZUYtAAAAAAAUKNl+ZNjt27c1e/ZsfffddwoICJCrq2u67WPHjs214gAAAAAAyM+yHbr37dunOnXqSJKOHDmS6wUBAAAAAFBQZDt0b9iwwYw6AAAAAAAocLJ9T3dISIiuXbt21/qkpCSFhITkSlEAAAAAABQE2Q7dc+fO1c2bN+9af/PmTc2bNy9XigIAAAAAoCDI8vDyq1evyjAMGYaha9euycXFxbotNTVVq1evlpeXlylFAgAAAACQH2U5dHt4eMhischisahy5cp3bbdYLPrggw9ytTgAAAAAAPKzLIfuDRs2yDAMPfvss/rqq69UvHhx6zYnJyeVK1dOpUqVMqVIAAAAAADyoyyH7saNG0uSTpw4obJly8pisZhWFAAAAAAABUG2J1IrV66cNm3apNdee00NGjTQmTNnJEn//e9/tWnTplwvEAAAAACA/Crbofurr75ScHCwChUqpF27dik5OVmSdOXKFY0aNSrXCwQAAAAAIL/KdugeOXKkpk2bphkzZsjR0dG6vmHDhtq1a1euFgcAAAAAQH6W7dB9+PBhPfXUU3etL1q0qC5fvpwbNQEAAAAAUCBkO3T7+Pjo2LFjd63ftGmTKlSokCtFAQAAAABQEGQ7dHfr1k29e/fWTz/9JIvForNnz2r+/Pnq27evevToYUaNAAAAAADkS1l+ZNgd/fv3V1pampo0aaIbN27oqaeekrOzs/r27au3337bjBoBAAAAAMiXsh26LRaLBg0apPfee0/Hjh3T9evXVa1aNRUpUsSM+gAAAAAAyLeyHbrvcHJyUrVq1XKzFgAAAAAACpQsh+6QkJAstZs9e3aOiwEAAAAAoCDJcuiOjIxUuXLlVLt2bRmGYWZNAAAAAAAUCFkO3T169NDChQt14sQJdenSRa+99pqKFy9uZm0AAAAAAORrWX5k2OTJk3Xu3Dm9//77WrlypXx9fdWmTRutXbuWnm8AAAAAADKQred0Ozs7q127dlq3bp0OHDig6tWrKzQ0VH5+frp+/bpZNQIAAAAAkC9lK3Sn29HOThaLRYZhKDU1NTdrAgAAAACgQMhW6E5OTtbChQv13HPPqXLlyoqNjdWkSZMUHx/Pc7oBAAAAAPibLE+kFhoaqqioKPn6+iokJEQLFy6Up6enmbUBAAAAAJCvZTl0T5s2TWXLllWFChW0ceNGbdy4McN2X3/9da4VBwAAAABAfpbl0N2pUydZLBYzawEAAAAAoEDJcuiOjIw0sQwAAAAAAAqeHM9eDgAAAAAA7o3QDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJgkT4TuyZMny8/PTy4uLqpfv762b9+epf2ioqJksVjUqlUrcwsEAAAAACAHbB66Fy1apPDwcA0bNky7du1SzZo1FRwcrAsXLtxzv5MnT6pv375q1KjRA6oUAAAAAIDssXnoHjt2rLp166YuXbqoWrVqmjZtmgoXLqzZs2dnuk9qaqo6dOigDz74QBUqVHiA1QIAAAAAkHU2Dd0pKSnauXOngoKCrOvs7OwUFBSkrVu3Zrrfhx9+KC8vL73xxhv/eI7k5GRdvXo13QIAAAAAwINg09CdmJio1NRUeXt7p1vv7e2t8+fPZ7jPpk2bNGvWLM2YMSNL54iIiFDRokWti6+v733XDQAAAABAVth8eHl2XLt2TR07dtSMGTPk6emZpX0GDBigK1euWJdTp06ZXCUAAAAAAH9wsOXJPT09ZW9vr4SEhHTrExIS5OPjc1f7uLg4nTx5Ui1atLCuS0tLkyQ5ODjo8OHDevTRR9Pt4+zsLGdnZxOqBwAAAADg3mza0+3k5KSAgACtX7/eui4tLU3r169XYGDgXe2rVKmi2NhYxcTEWJeWLVvqmWeeUUxMDEPHAQAAAAB5ik17uiUpPDxcnTt3Vt26dVWvXj2NHz9eSUlJ6tKliySpU6dOKl26tCIiIuTi4qLHH3883f4eHh6SdNd6AAAAAABszeahu23btvr11181dOhQnT9/XrVq1dKaNWusk6vFx8fLzi5f3XoOAAAAAICkPBC6JSksLExhYWEZbouOjr7nvpGRkblfEAAAAAAAuYAuZAAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkDrYuAAAAAABQsPjP9b+v/WM7x+ZSJbZHTzcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEgdbFwAAAADAXP5z/e9r/9jOsblUCfDwoacbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkzjYugCgoPGf639f+8d2js2lSgAAAADYGj3dAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmMTB1gUAAID8yX+uf473je0cm4uVAACQd9HTDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkcbF0AAAAAACD3+fVfdV/7n3TJpUIecvR0AwAAAABgkjwRuidPniw/Pz+5uLiofv362r59e6ZtZ8yYoUaNGqlYsWIqVqyYgoKC7tkeAAAAAABbsXnoXrRokcLDwzVs2DDt2rVLNWvWVHBwsC5cuJBh++joaLVr104bNmzQ1q1b5evrq6ZNm+rMmTMPuHIAAAAAAO7N5qF77Nix6tatm7p06aJq1app2rRpKly4sGbPnp1h+/nz5ys0NFS1atVSlSpVNHPmTKWlpWn9+vUPuHIAAAAAAO7NpqE7JSVFO3fuVFBQkHWdnZ2dgoKCtHXr1iwd48aNG7p165aKFy9uVpkAAAAAAOSITWcvT0xMVGpqqry9vdOt9/b21qFDh7J0jH79+qlUqVLpgvtfJScnKzk52fr66tWrOS8YAAAAAIBsyNePDPv4448VFRWl6OhoubhkPJ99RESEPvjggwdcGQAAAPAnHt0EPLxsOrzc09NT9vb2SkhISLc+ISFBPj4+99z3008/1ccff6z/+7//U40aNTJtN2DAAF25csW6nDp1KldqBwAAAADgn9g0dDs5OSkgICDdJGh3JkULDAzMdL8xY8ZoxIgRWrNmjerWrXvPczg7O8vd3T3dAgAAAADAg2Dz4eXh4eHq3Lmz6tatq3r16mn8+PFKSkpSly5dJEmdOnVS6dKlFRERIUkaPXq0hg4dqgULFsjPz0/nz5+XJBUpUkRFihSx2fsAAAAAAODvbB6627Ztq19//VVDhw7V+fPnVatWLa1Zs8Y6uVp8fLzs7P7skJ86dapSUlL08ssvpzvOsGHDNHz48AdZOgAAAAAA92Tz0C1JYWFhCgsLy3BbdHR0utcnT540vyAAAAAAAHKBTe/pBgAAAACgICN0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGCSPDGRGpDb/Pqvuq/9T7rkUiEAAAAAHmr0dAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBIHWxcAAAByxq//qvva/6RLLhUCAAAyRU83AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJskToXvy5Mny8/OTi4uL6tevr+3bt9+z/ZIlS1SlShW5uLjI399fq1evfkCVAgAAAACQdTYP3YsWLVJ4eLiGDRumXbt2qWbNmgoODtaFCxcybL9lyxa1a9dOb7zxhnbv3q1WrVqpVatW2rdv3wOuHAAAAACAe7N56B47dqy6deumLl26qFq1apo2bZoKFy6s2bNnZ9h+woQJev755/Xee++patWqGjFihOrUqaNJkyY94MoBAAAAALg3B1uePCUlRTt37tSAAQOs6+zs7BQUFKStW7dmuM/WrVsVHh6ebl1wcLCWL1+eYfvk5GQlJydbX1+5ckWSdPXq1fus3nxpyTfua/+rFuO+9k+9mZrzc9v4+try2t3PdZPy97Wz5c+cZNtrd98/c/ngd5JZuHY5x9+JnOPvRM7d198J/r3mGH9jc46fu5x7mH/XZcWdGg3jH66TYUNnzpwxJBlbtmxJt/69994z6tWrl+E+jo6OxoIFC9Ktmzx5suHl5ZVh+2HDhhmSWFhYWFhYWFhYWFhYWFhyfTl16tQ9c69Ne7ofhAEDBqTrGU9LS9OlS5f0yCOPyGKx2LCy+3P16lX5+vrq1KlTcnd3t3U5+QrXLue4djnDdcs5rl3Oce1yjmuXM1y3nOPa5RzXLue4dvfPMAxdu3ZNpUqVumc7m4ZuT09P2dvbKyEhId36hIQE+fj4ZLiPj49Ptto7OzvL2dk53ToPD4+cF53HuLu7848kh7h2Oce1yxmuW85x7XKOa5dzXLuc4brlHNcu57h2Oce1uz9Fixb9xzY2nUjNyclJAQEBWr9+vXVdWlqa1q9fr8DAwAz3CQwMTNdektatW5dpewAAAAAAbMXmw8vDw8PVuXNn1a1bV/Xq1dP48eOVlJSkLl26SJI6deqk0qVLKyIiQpLUu3dvNW7cWJ999pmaN2+uqKgo7dixQ9OnT7fl2wAAAAAA4C42D91t27bVr7/+qqFDh+r8+fOqVauW1qxZI29vb0lSfHy87Oz+7JBv0KCBFixYoMGDB2vgwIGqVKmSli9frscff9xWb8EmnJ2dNWzYsLuGzuOfce1yjmuXM1y3nOPa5RzXLue4djnDdcs5rl3Oce1yjmv34FgM45/mNwcAAAAAADlh03u6AQAAAAAoyAjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3flMWlqaUlNTbV0GHkI86AAP0rlz53TgwAFbl5Ev3fkbwb/Z7Llx44ZSUlJsXUa+dPr0ae3evdvWZeAhk5aWprS0NFuXAWQJoTsfOXDggDp16qTg4GD16NFDW7ZssXVJ+QYfVORMUlKSrl27pqtXr8pisdi6nHzl0qVLOnTokI4ePcp/5LPpzJkz8vf31+DBg7Vjxw5bl5OvxMTEqFWrVrpx4wb/ZrNh3759atOmjbZt26bk5GRbl5Ov7N+/Xw0aNNCXX34pSYSgbDh9+rQWL16sr7/+WrGxsbYuJ185cOCAXn/9dQUFBenNN99UVFSUrUsqMPjA1hyE7nzi8OHDatCggVJTU/XEE09o69at6t27tyZOnGjr0vK8I0eOaPz48Tp37pytS8lXDhw4oP/85z9q3Lixqlatqvnz50vil3FW7Nu3T0FBQWrTpo38/f01ZswYPvjJhqNHj+rKlSu6cuWKPv/8c+3atcu6jZ+/zO3Zs0cNGjRQ9erVVbhwYet6rtm97d+/X40aNVKZMmVUvnx5OTs727qkfGPPnj2qV6+eHBwctGDBAl24cEF2dvzXMitiY2P15JNP6pNPPlFoaKgGDRqkuLg4W5eVLxw6dEhPPvmknJyc9OKLLyo+Pl5DhgzR22+/bevS8pUjR46oX79+6tKliyZMmKCjR49KkiwWC383zGAgz0tLSzMGDhxotGnTxrru6tWrxsiRI41atWoZo0ePtmF1edvRo0eN4sWLGxaLxRgwYIDx66+/2rqkfGH//v3GI488YrzzzjvG/PnzjfDwcMPR0dHYvXu3rUvL8+5cu759+xr79+83Pv30U8NisRjx8fG2Li3fuHjxotGyZUvjiy++MOrUqWN06NDB2Ldvn2EYhpGammrj6vKmPXv2GK6ursZ7772Xbn1ycrKNKsofrl+/bjRt2tTo0aOHdd3BgweN3bt3G7/88osNK8v7YmJijEKFChkDBw40fv31V6N69erGyJEjjbS0NCMtLc3W5eVpJ0+eNEqXLm3079/fuH79urF69WrDx8fH+Omnn2xdWp73+++/Gx06dDB69eplXXfz5k2jdu3ahsViMdq1a2fD6vKP/fv3G0WLFjWef/55o3Xr1kbRokWNoKAgY8aMGdY2/DvOXQ62Dv34ZxaLRWfPntX58+et69zc3NSrVy+5uLgoKipKpUuXVocOHWxYZd6TlJSkiIgItWzZUk888YTCwsJ0+/Ztvf/++/L09LR1eXnWpUuX9M4776hDhw4aO3asJKl9+/batWuXZs+erYkTJ8owDIauZiAxMVE9evTQa6+9pk8++USSVLVqVX333Xc6ffq0Ll68qEceeUS+vr42rjTvSk1NVWpqqg4dOqQpU6aoRIkSioiI0IQJE7R//36VLFlSS5cutXWZecr58+cVHBysJ5980jqqom/fvjp69Kji4uLUvXt3Pf/886pSpYqtS81zHBwcdOPGDXXr1k2pqalq3ry59daQ6tWrq2vXrnrjjTdsXWaes3fvXtWvX1/vvvuuPvroI6Wlpalq1ar63//+p0GDBkkSfyfuYe3atapUqZJGjRoli8WiZs2aqU6dOoqJidGhQ4fk6+urZ555xtZl5knOzs46f/68KlWqJEn6/fff5eLioueee04VKlTQ4cOH9emnn6pv3742rjTvSklJUUREhNq0aaPp06dLko4dO6bBgwdr1qxZunHjhnr16sW/31zGGKA8zvj/wzvq1Kmj1NRUHT582LrNzc1NISEhql27tqZMmaIbN27Yqsw8yc7OTgEBAXr++ecVGhqqqKgoffrppxozZowSExNtXV6edevWLV2+fFkvv/yypD/vzytfvrwuXbokSfwizoTFYtHzzz+vnj17WteNHDlSa9euVWhoqFq0aKFu3bpp06ZNNqwyb7Ozs1OJEiX0xBNPaN++fXrppZc0fPhwLVu2TLGxsXrxxRdtXWKeFBgYqIsXL+p///ufXnzxRcXGxqpKlSpq0qSJJk6cqE8//VTx8fG2LjPPuXz5sg4fPqzExES99957kqSZM2dq8eLFatSokQYPHsyHPBlITk7W+++/bw3cdnZ2GjlypI4cOaKpU6dK4u/EvRiGofj4eMXExEiSPvroI3377bdasmSJJk2apFdffVWRkZE2rTEvMgzDOuFhXFycbt++LRcXF505c0aLFi1S8+bNVa1aNa1evdrWpeZpTk5OSkhIsP4bNQxDFStW1JgxY1SlShUtXbpUK1eutHGVBZAtu9mRdceOHTM8PT2NkJAQ49q1a4Zh/DnsIz4+3rBYLMa3335ryxLzpOvXr6d7HRUVZVgsFqNv375GYmKiYRh/DFc9fvy4LcrLs44cOWL9OiUlxTAMwxg8eLDRsWPHdO3u/CziT1evXrV+vXDhQsNisRiLFi0yLl68aGzcuNF44oknjOHDh9uwwvyhU6dORv/+/Q3DMIw33njDKFasmFGtWjUjJCSEIZgZOHv2rNGpUyejUKFCxnPPPWf9/WYYhjF//nzDw8PDWL16tQ0rzJvS0tKMV1991QgLCzNefPFFY82aNdZtp06dMl577TXjrbfeMm7fvs1Qy3tIS0szLl++bLRq1cpo06YN1+sfHD9+3GjQoIFRsWJFo3Xr1obFYjGWL19upKWlGQkJCUavXr2Mp59+2khMTOQ6ZmDTpk2GnZ2d8dRTTxkdO3Y0XF1dja5duxqGYRixsbGGm5ubcejQIa5dBm7fvm2kpKQYXbp0MV5++WXj999/N9LS0qy3bsXFxRmBgYFG27ZtbVxpwUNPdz7x6KOPavHixZo/f7769++vxMRE6ydUjo6OqlGjhooWLWrjKvMeV1dXSX8MWTUMQ23bttWCBQv02WefafTo0Tp79qz69u2rvn37MlLgL+4M20pLS5Ojo6OkPz4JvXDhgrVNRESEpk+frtu3b9ukxrzKzc3N+nVgYKB27NihNm3aqHjx4nrqqafk5eWlnTt32rDCvM34/6N7nn32WTk7Oys0NFSrV6/Wzp07NXLkSG3cuFFz5szR77//buNK85aSJUsqIiJCffr0Uf/+/fXII49Yr2X79u3l6empDRs22LjKvMdisejdd9/VnDlztGrVqnRPGihTpoy8vb114MAB2dnZ0XN7DxaLRUWLFlXHjh21ZMkSbdu2jet1D+XLl9eXX36pjz76SI8//rhat26tf//737JYLPLy8lKpUqX022+/ydXVleuYgYYNG2rbtm0qW7asnJ2dNWbMGM2YMUOSdPz4cZUpU0Y+Pj5cu7+4M5mrvb29HB0d1blzZy1btkxffPGFLBaL7OzslJqaqgoVKigiIkJLlizR/v37bVx1wcI93fnIM888oyVLluiVV17RuXPn1KZNG9WoUUPz5s3ThQsXuE/0Huzt7WUYhtLS0vTqq6/KYrGoY8eOWrFiheLi4vTzzz+nm+0Xf7Czs0t3X96dWWmHDh2qkSNHavfu3XJw4NdIZsqVK6dy5cpJ+uMDjJSUFBUpUkQ1atSwcWV5152ftfLly6tLly7y9vbWN998o/Lly6t8+fKyWCyqWbOmXFxcbFxp3lOqVCn179/fem3uzEB76dIllShRQrVq1bJtgXlU3bp19e2336px48aaPn26KlSooOrVq0v643abypUr6/bt29YPIJG5F198Uc8995ymTp2qOnXqqFChQrYuKc+68ztt5syZ2rFjh1JSUuTk5CRJSkhIkJ+fH0+9uIcnnnhC8+bNuytY//jjj/L29iZw/8WRI0e0cuVKtW/fXiVLlpQkNW7cWKNHj9Y777yjwoULq2vXrrK3t5f0R+fBY489Zu24Qu7gf8v5TIsWLbRlyxaFh4erX79+cnBwkL29vVatWqUyZcrYurw87a/3rrRt21bTp09XTEyMdu3aJX9/fxtXl3fdCd0ODg7y9fW13he/Y8cO1axZ09bl5Rt2dnYaNWqUtm7dqhEjRti6nDwvMDBQM2fOVN26dVWjRg3rz2GrVq1sXVqe5u7unu61xWLRxIkTlZiYqIYNG9qoqryvUaNGio6OVrt27RQSEiJ/f3+lpKRoxYoV2rRpE4E7i5ycnPTMM88oIiJCV65cIXRnQYMGDdS3b19NmDBBPj4+2rdvn+bMmaMffviB0PMP/hqsY2NjNW3aNH355Zf64Ycf7vpd+LA6duyYAgMD9dtvv+nixYsKDw+3Tibco0cPJSUl6c0339Qvv/yi//znPypXrpyWLFmiW7du8fOXywjd+VCdOnW0YsUKXbp0SdeuXVPJkiWZjTuLLBaLUlNT9d5772nDhg2KiYkhcP+DO73bjo6OmjFjhtzd3bVp0ybVqVPHxpXlH0uWLNHGjRsVFRWldevWWYfvI3OOjo56/fXXrT9/9FpkX1RUlDZs2KAlS5Zo/fr11lEXyNhTTz2l77//Xl9++aW2bdumSpUqadOmTXr88cdtXVq+cOeDse7du2vp0qXcApJF1apV07Jly9StWzfZ2dmpdOnS2rhxI/83yYbk5GQdO3ZMly5d0o8//shosv8vs6f4vPfeeypRooQKFy6swYMHy8/PT/369dOcOXPk5uamq1evauXKlSpRooSt30KBYjEMnn6Oh0tqaqoiIyMVEBDAcMts2LFjh+rVq6d9+/apWrVqti4nX9m/f78+/PBDDR8+XFWrVrV1OXhI7N27VwMHDtTo0aOtw6WRNXee2nDnQx9knfH/Z5imlyx7Ll26pFu3bsnZ2VkeHh62LiffSU5O1u3bt/m5+4ubN29qzpw5euSRR9S2bVstXrxYr776qvr27WsN3necPHlS8fHxunHjhvz9/VW6dGkbVl4wEbrxUDJ4fmiOJCUl8Qcth27dusUQVTxwf71PFADwcPn7/9sWLVqkdu3a6d1331W/fv3k6emp27dv6+zZsypbtqwNKy34GF6OhxKBO2cI3DlH4IYtELgB4OH116f42NnZqW3btjIMQ+3bt5fFYlGfPn306aef6pdfftG8efNUuHBh/o9sEnq6AQAAAKAAMwxDhmHIzs5OixYtUseOHVWhQgXrU3y45dJchG4AAAAAKODuxD6LxaImTZooJiZG0dHRTNz3ADC8HAAAAAAKOJ7iYztMywkAAAAAD4nq1atr165dPF7tAWJ4OQAAAAA8JHiKz4NHTzcAAAAAPCQI3A8eoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAgIeUxWLR8uXLbV0GAAAFGqEbAIAC6vz583r77bdVoUIFOTs7y9fXVy1atND69ettXRoAAA8NB1sXAAAAct/JkyfVsGFDeXh46JNPPpG/v79u3bqltWvXqmfPnjp06JCtSwQA4KFATzcAAAVQaGioLBaLtm/frtatW6ty5cqqXr26wsPDtW3btgz36devnypXrqzChQurQoUKGjJkiG7dumXdvmfPHj3zzDNyc3OTu7u7AgICtGPHDknSL7/8ohYtWqhYsWJydXVV9erVtXr16gfyXgEAyMvo6QYAoIC5dOmS1qxZo48++kiurq53bffw8MhwPzc3N0VGRqpUqVKKjY1Vt27d5Obmpvfff1+S1KFDB9WuXVtTp06Vvb29YmJi5OjoKEnq2bOnUlJS9MMPP8jV1VUHDhxQkSJFTHuPAADkF4RuAAAKmGPHjskwDFWpUiVb+w0ePNj6tZ+fn/r27auoqChr6I6Pj9d7771nPW6lSpWs7ePj49W6dWv5+/tLkipUqHC/bwMAgAKB4eUAABQwhmHkaL9FixapYcOG8vHxUZEiRTR48GDFx8dbt4eHh6tr164KCgrSxx9/rLi4OOu2Xr16aeTIkWrYsKGGDRumvXv33vf7AACgICB0AwBQwFSqVEkWiyVbk6Vt3bpVHTp00AsvvKBvvvlGu3fv1qBBg5SSkmJtM3z4cO3fv1/NmzfX999/r2rVqmnZsmWSpK5du+r48ePq2LGjYmNjVbduXX3++ee5/t4AAMhvLEZOPw4HAAB5VrNmzRQbG6vDhw/fdV/35cuX5eHhIYvFomXLlqlVq1b67LPPNGXKlHS91127dtXSpUt1+fLlDM/Rrl07JSUlacWKFXdtGzBggFatWkWPNwDgoUdPNwAABdDkyZOVmpqqevXq6auvvtLRo0d18OBBTZw4UYGBgXe1r1SpkuLj4xUVFaW4uDhNnDjR2ostSTdv3lRYWJiio6P1yy+/aPPmzfr5559VtWpVSVKfPn20du1anThxQrt27dKGDRus2wAAeJgxkRoAAAVQhQoVtGvXLn300Ud69913de7cOZUoUUIBAQGaOnXqXe1btmypd955R2FhYUpOTlbz5s01ZMgQDR8+XJJkb2+vixcvqlOnTkpISJCnp6f+85//6IMPPpAkpaamqmfPnjp9+rTc3d31/PPPa9y4cQ/yLQMAkCcxvBwAAAAAAJMwvBwAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADDJ/wPaZS8daLitNgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class_labels = list(report.keys())[:-4]\n",
        "metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "metric_values = {metric: [] for metric in metrics}\n",
        "\n",
        "for label in class_labels:\n",
        "    for metric in metrics:\n",
        "        metric_values[metric].append(report[label][metric])\n",
        "\n",
        "x = np.arange(len(class_labels))\n",
        "width = 0.2\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax.bar(x + (i * width), metric_values[metric], width, align='center', label=metric)\n",
        "\n",
        "ax.set_xlabel('Class')\n",
        "ax.set_ylabel('Metric Value')\n",
        "ax.set_title('Metrics by Class')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(class_labels, rotation=45)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqY9D8Cts5dw",
        "outputId": "416e3a81-1882-4ae7-8c60-a193748ed7c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 68, 128)           459264    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 68, 64)            49408     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 32)                12416     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 11)                363       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 521,451\n",
            "Trainable params: 521,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR8xGLa8x7Rt"
      },
      "outputs": [],
      "source": [
        "model.save('keras_lstm_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.wrappers'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[74], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscikit_learn\u001b[39;00m \u001b[39mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def create_model(optimizer='adam', activation='sigmoid', dropout_rate=0.0, lstm_units=32):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "                units=lstm_units,\n",
        "                activation=activation,\n",
        "                recurrent_activation='hard_sigmoid',\n",
        "                return_sequences=True\n",
        "                ))\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "    model.add(LSTM(units=lstm_units))\n",
        "    model.add(Dense(11, activation=activation))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', KerasClassifier(build_fn=create_model, verbose=1))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'model__optimizer': ['adam', 'rmsprop', 'sgd', 'adagrad', 'adadelta', 'adamax', 'nadam'],\n",
        "    'model__activation': ['relu', 'sigmoid', 'softmax', 'tanh', 'linear', 'elu', 'selu', 'softplus', 'hard_sigmoid'],\n",
        "    'model__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
        "    'model__lstm_units': [16, 32, 64, 128],\n",
        "    'model__batch_size': [16, 32, 64]\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train, callbacks=[EarlyStopping(patience=5)])\n",
        "\n",
        "# Print the best results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hVuQSjzp9DMz",
        "CWQobiKu4-kG",
        "RYNB6AkK5C50",
        "4_6uN2mHgK4P"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
